<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Processing Stages - Detailed Explanation</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            min-height: 100vh;
            padding: 20px;
            color: #2c3e50;
        }
        
        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: #ffffff;
            border-radius: 12px;
            padding: clamp(20px, 3vw, 40px);
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
            border: 1px solid #e1e8ed;
        }
        
        .header {
            text-align: center;
            margin-bottom: clamp(20px, 4vw, 40px);
            padding-bottom: 20px;
            border-bottom: 2px solid #f0f0f0;
        }
        
        h1 {
            color: #2c3e50;
            margin-bottom: clamp(10px, 2vw, 15px);
            font-size: clamp(1.8em, 4vw, 2.5em);
            font-weight: 700;
        }
        
        .subtitle {
            color: #7f8c8d;
            font-size: clamp(14px, 2.5vw, 18px);
            font-weight: 400;
            line-height: 1.5;
        }
        
        .input-section {
            background: #f8f9fa;
            padding: clamp(20px, 3vw, 25px);
            border-radius: 8px;
            margin-bottom: clamp(20px, 4vw, 30px);
            border-left: 4px solid #6c757d;
        }
        
        .input-text {
            font-size: clamp(20px, 4vw, 26px);
            font-weight: 600;
            color: #2c3e50;
            text-align: center;
            word-wrap: break-word;
            margin-bottom: 10px;
        }
        
        .input-context {
            font-size: clamp(12px, 2vw, 14px);
            color: #6c757d;
            text-align: center;
            font-style: italic;
        }
        
        .blank {
            background: #dc3545;
            color: white;
            padding: 4px 12px;
            border-radius: 4px;
            display: inline-block;
            animation: pulse 2s infinite;
            font-weight: 700;
        }
        
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.8; }
        }
        
        .stages-container {
            display: grid;
            gap: clamp(15px, 3vw, 25px);
        }
        
        .stage {
            background: #ffffff;
            border-radius: 8px;
            padding: clamp(20px, 3vw, 30px);
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
            border: 1px solid #e9ecef;
            transition: all 0.4s ease;
            opacity: 0.4;
            transform: translateY(10px);
        }
        
        .stage.active {
            opacity: 1;
            transform: translateY(0);
            border-color: #6c757d;
            box-shadow: 0 4px 16px rgba(108, 117, 125, 0.15);
        }
        
        .stage-header {
            display: flex;
            align-items: flex-start;
            margin-bottom: clamp(15px, 2vw, 20px);
            flex-wrap: wrap;
        }
        
        .stage-number {
            background: #6c757d;
            color: white;
            width: clamp(35px, 6vw, 45px);
            height: clamp(35px, 6vw, 45px);
            border-radius: 6px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 700;
            margin-right: clamp(12px, 2vw, 18px);
            font-size: clamp(14px, 2.5vw, 18px);
            flex-shrink: 0;
        }
        
        .stage-title-container {
            flex: 1;
        }
        
        .stage-title {
            font-size: clamp(1.2em, 3vw, 1.6em);
            font-weight: 700;
            color: #2c3e50;
            margin-bottom: 5px;
        }
        
        .stage-subtitle {
            font-size: clamp(12px, 2vw, 14px);
            color: #6c757d;
            font-weight: 500;
        }
        
        .stage-description {
            color: #495057;
            line-height: 1.6;
            margin-bottom: clamp(15px, 2vw, 20px);
            font-size: clamp(14px, 2.2vw, 16px);
        }
        
        .technical-details {
            background: #f8f9fa;
            border-left: 3px solid #6c757d;
            padding: clamp(12px, 2vw, 15px);
            margin: clamp(10px, 2vw, 15px) 0;
            border-radius: 4px;
        }
        
        .technical-details h4 {
            color: #495057;
            font-size: clamp(13px, 2vw, 15px);
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .technical-details p {
            color: #6c757d;
            font-size: clamp(12px, 2vw, 14px);
            line-height: 1.5;
        }
        
        .visual-element {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 6px;
            padding: clamp(15px, 2vw, 20px);
            margin: 15px 0;
        }
        
        .tokens {
            display: flex;
            gap: clamp(6px, 1.5vw, 8px);
            flex-wrap: wrap;
            justify-content: center;
            margin-bottom: 15px;
        }
        
        .token {
            background: #ffffff;
            border: 2px solid #6c757d;
            padding: clamp(6px, 1.5vw, 8px) clamp(12px, 2vw, 16px);
            border-radius: 4px;
            font-weight: 600;
            transition: all 0.2s ease;
            font-size: clamp(12px, 2vw, 14px);
            white-space: nowrap;
            color: #2c3e50;
        }
        
        .token:hover {
            background: #6c757d;
            color: white;
        }
        
        .token-info {
            font-size: clamp(10px, 1.8vw, 12px);
            color: #6c757d;
            text-align: center;
            margin-top: 10px;
        }
        
        .embedding-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(25px, 1fr));
            gap: clamp(2px, 0.5vw, 3px);
            max-width: clamp(300px, 70vw, 400px);
            margin: 0 auto 15px;
        }
        
        .embedding-cell {
            width: clamp(22px, 4vw, 28px);
            height: clamp(22px, 4vw, 28px);
            border: 1px solid #dee2e6;
            border-radius: 2px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: clamp(7px, 1.2vw, 9px);
            color: #495057;
            font-weight: 600;
            background: #ffffff;
        }
        
        .attention-matrix {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: clamp(3px, 1vw, 4px);
            max-width: clamp(200px, 50vw, 250px);
            margin: 0 auto 15px;
        }
        
        .attention-cell {
            width: clamp(35px, 7vw, 45px);
            height: clamp(35px, 7vw, 45px);
            border: 1px solid #dee2e6;
            border-radius: 4px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: #495057;
            font-weight: 600;
            transition: transform 0.2s ease;
            font-size: clamp(9px, 1.8vw, 11px);
            background: #ffffff;
        }
        
        .attention-cell:hover {
            transform: scale(1.05);
            border-color: #6c757d;
        }
        
        .layer-diagram {
            display: flex;
            flex-direction: column;
            align-items: center;
            max-width: 350px;
            margin: 0 auto;
        }
        
        .layer-box {
            width: 100%;
            padding: clamp(10px, 2vw, 12px);
            margin: 4px 0;
            border: 1px solid #6c757d;
            border-radius: 4px;
            text-align: center;
            font-size: clamp(12px, 2vw, 14px);
            font-weight: 600;
            color: #2c3e50;
            background: #ffffff;
            transition: background-color 0.2s ease;
        }
        
        .layer-box:hover {
            background: #f8f9fa;
        }
        
        .layer-arrow {
            font-size: clamp(14px, 2.5vw, 16px);
            color: #6c757d;
            margin: 2px 0;
        }
        
        .predictions {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(110px, 1fr));
            gap: clamp(8px, 2vw, 12px);
            margin-bottom: 15px;
        }
        
        .prediction {
            background: #ffffff;
            border: 1px solid #6c757d;
            color: #2c3e50;
            padding: clamp(10px, 2vw, 12px);
            border-radius: 4px;
            text-align: center;
            font-weight: 600;
            transition: all 0.2s ease;
            font-size: clamp(12px, 2.2vw, 14px);
        }
        
        .prediction:hover {
            background: #6c757d;
            color: white;
        }
        
        .prediction-word {
            font-weight: 700;
            margin-bottom: 4px;
        }
        
        .probability {
            font-size: clamp(10px, 1.8vw, 11px);
            color: #6c757d;
            font-weight: 500;
        }
        
        .prediction:hover .probability {
            color: rgba(255, 255, 255, 0.8);
        }
        
        .controls {
            text-align: center;
            margin-top: clamp(25px, 4vw, 40px);
            padding-top: 20px;
            border-top: 1px solid #e9ecef;
            display: flex;
            flex-wrap: wrap;
            gap: 12px;
            justify-content: center;
        }
        
        .btn {
            background: #6c757d;
            color: white;
            border: none;
            padding: clamp(12px, 2vw, 14px) clamp(20px, 4vw, 28px);
            border-radius: 4px;
            font-size: clamp(14px, 2.5vw, 16px);
            font-weight: 600;
            cursor: pointer;
            transition: all 0.2s ease;
            white-space: nowrap;
        }
        
        .btn:hover {
            background: #5a6268;
            transform: translateY(-1px);
        }
        
        .btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
        }
        
        .progress-bar {
            width: 100%;
            height: 4px;
            background: #e9ecef;
            border-radius: 2px;
            margin: 20px 0;
            overflow: hidden;
        }
        
        .progress-fill {
            height: 100%;
            background: #6c757d;
            border-radius: 2px;
            transition: width 0.6s ease;
            width: 0%;
        }
        
        .step-info {
            background: #e9ecef;
            padding: clamp(10px, 2vw, 15px);
            border-radius: 4px;
            margin-top: 15px;
            text-align: center;
        }
        
        .step-info p {
            color: #495057;
            font-size: clamp(11px, 2vw, 13px);
            font-weight: 500;
            margin: 0;
        }
        
        @media (max-width: 768px) {
            .stage-header {
                flex-direction: column;
                align-items: center;
                text-align: center;
            }
            
            .stage-number {
                margin-right: 0;
                margin-bottom: 10px;
            }
            
            .predictions {
                grid-template-columns: repeat(auto-fit, minmax(90px, 1fr));
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>🔍 How Large Language Models Process Text</h1>
            <p class="subtitle">A step-by-step breakdown of the internal mechanisms that enable LLMs to understand and predict text</p>
        </div>
        
        <div class="input-section">
            <div class="input-text">
                Input: "This is an <span class="blank">______</span>"
            </div>
            <div class="input-context">
                The model needs to predict the most likely word to fill the blank based on context
            </div>
        </div>
        
        <div class="progress-bar">
            <div class="progress-fill" id="progressFill"></div>
        </div>
        
        <div class="stages-container" id="stagesContainer">
            <div class="stage" data-stage="1">
                <div class="stage-header">
                    <div class="stage-number">1</div>
                    <div class="stage-title-container">
                        <div class="stage-title">Tokenization</div>
                        <div class="stage-subtitle">Converting text into processable units</div>
                    </div>
                </div>
                <div class="stage-description">
                    The input text is broken down into tokens—discrete units that the model can process. These can be whole words, parts of words (subwords), or individual characters, depending on the tokenization strategy. Special tokens like [CLS] (classification) and [MASK] are added to provide context and mark positions.
                </div>
                <div class="technical-details">
                    <h4>Technical Details:</h4>
                    <p>Modern LLMs typically use subword tokenization methods like BPE (Byte Pair Encoding) or SentencePiece. This approach balances vocabulary size with the ability to handle rare or out-of-vocabulary words by breaking them into known subword components.</p>
                </div>
                <div class="visual-element">
                    <div class="tokens">
                        <div class="token">[CLS]</div>
                        <div class="token">This</div>
                        <div class="token">is</div>
                        <div class="token">an</div>
                        <div class="token">[MASK]</div>
                        <div class="token">[SEP]</div>
                    </div>
                    <div class="token-info">
                        Each token is assigned a unique numerical ID from the model's vocabulary (typically 30k-50k tokens)
                    </div>
                </div>
            </div>
            
            <div class="stage" data-stage="2">
                <div class="stage-header">
                    <div class="stage-number">2</div>
                    <div class="stage-title-container">
                        <div class="stage-title">Embedding & Positional Encoding</div>
                        <div class="stage-subtitle">Converting tokens to high-dimensional vectors</div>
                    </div>
                </div>
                <div class="stage-description">
                    Each token ID is converted into a dense vector representation (embedding) that captures semantic meaning. Additionally, positional encodings are added to preserve the order of tokens, as the attention mechanism itself is position-agnostic. These embeddings are learned during training and capture rich semantic relationships.
                </div>
                <div class="technical-details">
                    <h4>Technical Details:</h4>
                    <p>Token embeddings are typically 768-dimensional (BERT-base) or 1024-dimensional (BERT-large). Positional encodings use sinusoidal functions or learned embeddings to encode position information. The final input to the model is: token embedding + positional encoding.</p>
                </div>
                <div class="visual-element">
                    <div class="embedding-grid" id="embeddingGrid"></div>
                    <div class="step-info">
                        <p>768-dimensional vectors representing semantic meaning and positional information</p>
                    </div>
                </div>
            </div>
            
            <div class="stage" data-stage="3">
                <div class="stage-header">
                    <div class="stage-number">3</div>
                    <div class="stage-title-container">
                        <div class="stage-title">Multi-Head Self-Attention</div>
                        <div class="stage-subtitle">Understanding relationships between tokens</div>
                    </div>
                </div>
                <div class="stage-description">
                    The core mechanism that allows the model to understand relationships between different parts of the input. Each token "attends" to all other tokens, learning which words are most relevant for understanding the current context. Multiple attention heads capture different types of relationships simultaneously.
                </div>
                <div class="technical-details">
                    <h4>Technical Details:</h4>
                    <p>Self-attention computes three matrices: Query (Q), Key (K), and Value (V). Attention weights are calculated as softmax(QK^T/√d). BERT-base uses 12 attention heads per layer, each focusing on different linguistic patterns like syntactic relationships, semantic similarity, or positional dependencies.</p>
                </div>
                <div class="visual-element">
                    <div class="attention-matrix" id="attentionMatrix"></div>
                    <div class="step-info">
                        <p>Attention weights showing how much each token focuses on others (darker = stronger attention)</p>
                    </div>
                </div>
            </div>
            
            <div class="stage" data-stage="4">
                <div class="stage-header">
                    <div class="stage-number">4</div>
                    <div class="stage-title-container">
                        <div class="stage-title">Transformer Block Processing</div>
                        <div class="stage-subtitle">Deep contextual understanding through multiple layers</div>
                    </div>
                </div>
                <div class="stage-description">
                    The embeddings pass through multiple transformer blocks (12 in BERT-base, 24 in BERT-large). Each block contains multi-head attention followed by a feed-forward network, with residual connections and layer normalization. Early layers capture syntactic patterns, while deeper layers learn semantic and contextual relationships.
                </div>
                <div class="technical-details">
                    <h4>Technical Details:</h4>
                    <p>Each transformer block applies: (1) Multi-head self-attention with residual connection, (2) Layer normalization, (3) Feed-forward network (typically 4x the embedding dimension), (4) Another residual connection and layer normalization. This architecture enables the model to build increasingly complex representations.</p>
                </div>
                <div class="visual-element">
                    <div class="layer-diagram">
                        <div class="layer-box">Multi-Head Self-Attention</div>
                        <div class="layer-arrow">↓</div>
                        <div class="layer-box">Add & Layer Norm</div>
                        <div class="layer-arrow">↓</div>
                        <div class="layer-box">Feed Forward Network</div>
                        <div class="layer-arrow">↓</div>
                        <div class="layer-box">Add & Layer Norm</div>
                    </div>
                    <div class="step-info">
                        <p>This structure repeats 12 times in BERT-base, creating deep contextual representations</p>
                    </div>
                </div>
            </div>
            
            <div class="stage" data-stage="5">
                <div class="stage-header">
                    <div class="stage-number">5</div>
                    <div class="stage-title-container">
                        <div class="stage-title">Classification Head & Prediction</div>
                        <div class="stage-subtitle">Converting representations to word probabilities</div>
                    </div>
                </div>
                <div class="stage-description">
                    The final hidden state for the [MASK] token is passed through a classification layer that projects it to the vocabulary size. A softmax function converts these logits into probabilities for each possible word. The model selects the word with the highest probability as its prediction.
                </div>
                <div class="technical-details">
                    <h4>Technical Details:</h4>
                    <p>The classification head is typically a linear layer that maps from the hidden dimension (768) to vocabulary size (~30k). The softmax function ensures all probabilities sum to 1. During training, the model learns to maximize the probability of correct words through backpropagation and gradient descent.</p>
                </div>
                <div class="visual-element">
                    <div class="predictions">
                        <div class="prediction">
                            <div class="prediction-word">apple</div>
                            <div class="probability">24.3%</div>
                        </div>
                        <div class="prediction">
                            <div class="prediction-word">example</div>
                            <div class="probability">19.7%</div>
                        </div>
                        <div class="prediction">
                            <div class="prediction-word">elephant</div>
                            <div class="probability">15.2%</div>
                        </div>
                        <div class="prediction">
                            <div class="prediction-word">idea</div>
                            <div class="probability">12.8%</div>
                        </div>
                        <div class="prediction">
                            <div class="prediction-word">object</div>
                            <div class="probability">9.4%</div>
                        </div>
                        <div class="prediction">
                            <div class="prediction-word">animal</div>
                            <div class="probability">7.1%</div>
                        </div>
                    </div>
                    <div class="step-info">
                        <p>The model outputs probability distributions over the entire vocabulary (~30,000 possible words)</p>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="controls">
            <button class="btn" onclick="startAnimation()">▶️ Start Animation</button>
            <button class="btn" onclick="nextStep()" id="nextBtn" disabled>➡️ Next Step</button>
            <button class="btn" onclick="resetAnimation()">🔄 Reset</button>
        </div>
    </div>

    <script>
        let currentStep = 0;
        let isAnimating = false;
        
        function generateEmbeddingGrid() {
            const grid = document.getElementById('embeddingGrid');
            grid.innerHTML = '';
            for (let i = 0; i < 48; i++) {
                const cell = document.createElement('div');
                cell.className = 'embedding-cell';
                const value = Math.random();
                if (value > 0.7) {
                    cell.style.background = '#495057';
                    cell.style.color = 'white';
                    cell.textContent = '1.0';
                } else if (value > 0.4) {
                    cell.style.background = '#adb5bd';
                    cell.textContent = '0.5';
                } else if (value > 0.2) {
                    cell.style.background = '#dee2e6';
                    cell.textContent = '0.2';
                } else {
                    cell.style.background = '#f8f9fa';
                    cell.textContent = '0.0';
                }
                grid.appendChild(cell);
            }
        }
        
        function generateAttentionMatrix() {
            const matrix = document.getElementById('attentionMatrix');
            matrix.innerHTML = '';
            const tokens = ['This', 'is', 'an', '[MASK]'];
            const attentionWeights = [
                [0.15, 0.25, 0.35, 0.25],
                [0.20, 0.40, 0.25, 0.15],
                [0.10, 0.15, 0.25, 0.50],
                [0.18, 0.22, 0.40, 0.20]
            ];
            
            for (let i = 0; i < 16; i++) {
                const cell = document.createElement('div');
                cell.className = 'attention-cell';
                const row = Math.floor(i / 4);
                const col = i % 4;
                const weight = attentionWeights[row][col];
                
                // Use grayscale based on attention weight
                const intensity = Math.floor((1 - weight) * 200 + 55);
                cell.style.background = `rgb(${intensity}, ${intensity}, ${intensity})`;
                if (weight > 0.3) {
                    cell.style.color = 'white';
                }
                cell.textContent = weight.toFixed(2);
                cell.title = `${tokens[row]} → ${tokens[col]}: ${weight.toFixed(3)}`;
                matrix.appendChild(cell);
            }
        }
        
        function updateProgress(step) {
            const progress = (step / 5) * 100;
            document.getElementById('progressFill').style.width = progress + '%';
        }
        
        function activateStage(stageNumber) {
            const stages = document.querySelectorAll('.stage');
            stages.forEach((stage, index) => {
                if (index < stageNumber) {
                    stage.classList.add('active');
                } else {
                    stage.classList.remove('active');
                }
            });
        }
        
        function startAnimation() {
            if (isAnimating) return;
            isAnimating = true;
            currentStep = 0;
            
            document.querySelector('button[onclick="startAnimation()"]').disabled = true;
            document.getElementById('nextBtn').disabled = false;
            
            nextStep();
        }
        
        function nextStep() {
            if (currentStep >= 5) return;
            
            currentStep++;
            activateStage(currentStep);
            updateProgress(currentStep);
            
            if (currentStep === 2) {
                setTimeout(generateEmbeddingGrid, 400);
            } else if (currentStep === 3) {
                setTimeout(generateAttentionMatrix, 400);
            }
            
            if (currentStep >= 5) {
                document.getElementById('nextBtn').disabled = true;
                setTimeout(() => {
                    isAnimating = false;
                    document.querySelector('button[onclick="startAnimation()"]').disabled = false;
                }, 600);
            }
        }
        
        function resetAnimation() {
            currentStep = 0;
            isAnimating = false;
            
            activateStage(0);
            updateProgress(0);
            
            document.querySelector('button[onclick="startAnimation()"]').disabled = false;
            document.getElementById('nextBtn').disabled = true;
            
            document.getElementById('embeddingGrid').innerHTML = '';
            document.getElementById('attentionMatrix').innerHTML = '';
        }
        
        // Initialize
        resetAnimation();
    </script>
</body>
</html>